{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install required libraries\n",
    "!pip install google-auth google-auth-httplib2 google-auth-oauthlib google-api-python-client\n",
    "!pip install -q -U google-generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import necessary libraries\n",
    "from google.oauth2 import service_account\n",
    "from googleapiclient.discovery import build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up your credentials and project details\n",
    "# SERVICE_ACCOUNT_FILE = 'ANON.json'  # Upload this file to Colab\n",
    "# PROJECT_ID = 'ANON'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authenticate with the service account\n",
    "# credentials = service_account.Credentials.from_service_account_file(\n",
    "# SERVICE_ACCOUNT_FILE\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openpyxl\n",
    "import google.generativeai as genai\n",
    "from google.colab import userdata\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the API Key\n",
    "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qmGkH0WztTIc"
   },
   "source": [
    "# Reading Excel Sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"Explain.xlsx\"\n",
    "workbook = openpyxl.load_workbook(path)\n",
    "sheet = workbook[\"All_data\"]  # There is only one sheet, so no ambiguity here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5uKQIYXct0_j"
   },
   "source": [
    "Get all the responses and scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_COLUMN = 8 # Specify the column from which input is read (A->1, B->2, etc., x->24)\n",
    "APPLY_FILTER = True # Whether to filter the input rows\n",
    "row_count =  1250 # sheet.max_row  # Get number of rows. If number of rows to read is already known (or sheet does not terminate where data terminates), place number here (+1) instead.\n",
    "# Filter structure column number: ([values], valid/invalid)\n",
    "#53: ([\"Mixed\"], False),\n",
    "INPUT_FILTER_DICTIONARY = { 4: ([\n",
    "                                \"3. Why do you think the approach you selected in the previous question will best support Jeremiah by assisting him with advocating for himself? <br/>\",\n",
    "                               \"3. Why do you think the approach you selected in the previous question will best support Alexis by assisting her with advocating for herself? <br/>\",\n",
    "                               \"11. Why do you think the approach you selected in the previous question will best support Alexis by assisting her with advocating for herself? <br/>\",\n",
    "                               \"11. Why do you think the approach you selected in the previous question will best support Jeremiah by assisting him with advocating for himself? <br/>\"],\n",
    "                                True)}\n",
    "# Function to check if a row matches the filter criteria\n",
    "def matches_filters(sheet, row, filter_dict):\n",
    "    for col, (valid_values, is_valid) in filter_dict.items():\n",
    "        cell_value = sheet.cell(row=row, column=col).value\n",
    "        if (cell_value in valid_values) != is_valid:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "# Read the input data\n",
    "inputs = []\n",
    "input_rows = []\n",
    "for i in range(2, row_count + 1):\n",
    "    if not APPLY_FILTER or matches_filters(sheet, i, INPUT_FILTER_DICTIONARY):\n",
    "        cell_value = sheet.cell(row=i, column=INPUT_COLUMN).value\n",
    "        if cell_value is not None:\n",
    "            inputs.append(cell_value)\n",
    "            input_rows.append(i)\n",
    "\n",
    "# Now `inputs` contains only the filtered data\n",
    "#inputs = [sheet.cell(row=i, column=INPUT_COLUMN).value  for i in range(2, row_count + 1)]  # Column number is set here. YOUR INPUT COLUMN NUMBER GOES HERE.\n",
    "#inputs = [x for x in inputs if x is not None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cKg2bQ7qvi_i"
   },
   "source": [
    "Scoring Prompt Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCORING_PROMPT_START = \"\"\"\n",
    "You are a tutor evaluator. Please assess a tutor’s response within a tutor training scenario involving a tutor instructing a middle school student to advocate for themselves when they are experiencing an inequity. Educators and tutors should apply the strategy of assisting students in identifying when they are experiencing an inequity and instruct them to advocate for themselves. The tutor is explaining the rationale behind their response. Assess and score the tutor’s response, as follows:\n",
    "\n",
    "\n",
    "-if the tutor’s response demonstrates that they recognize that the student needs support in advocating for themselves and encourages the student to act, score with a 1. Sample responses scoring a 1 include: “It practices critical hope. It encourages her to advocate for herself by speaking to the teacher about it, and I offer my own help to assist her”; “It prepares Alexis to solve the problem herself by practicing in a low stress environment. It would make her more confident to talk to her teacher and do better in class.”\n",
    "\n",
    "\n",
    "-if the tutor's response does not demonstrate that the tutor understands that the student needs support in advocating for themselves, score with a 0. Sample responses scoring a 0 include: “That would be the right approach because I am helping the student solve the problem rationally\"; \"This can let him feel supported and at the same time, gives him a solution of this problem\"; \"Because it teaches Alexis a possible way to avoid similar problems in the future; and \"Discussing the plan with the student will provide them how to stand up for the inequity.\"\n",
    "\n",
    "\n",
    "Response Start ---\n",
    "\"\"\"\n",
    "\n",
    "SCORING_FORMAT_PROMPT = \"\"\"\n",
    "--- Response End. Given the earlier response, please return a JSON string following the format, {\"Rationale\": \"your reasoning here\", \"Score\":0/1}.\n",
    "\"\"\"\n",
    "\n",
    "GEMINI_SYSTEM_PROMPT = \"\"\"\n",
    "You are a tutor evaluator. Please assess a tutor’s response within a tutor training scenario involving a tutor instructing a middle school student to advocate for themselves when they are experiencing an inequity. Educators and tutors should apply the strategy of assisting students in identifying when they are experiencing an inequity and instruct them to advocate for themselves. The tutor is explaining the rationale behind their response. Assess and score the tutor’s response, as follows:\n",
    "\n",
    "-if the tutor’s response demonstrates that they recognize that the student needs support in advocating for themselves and encourages the student to act, score with a 1. Sample responses scoring a 1 include: “It practices critical hope. It encourages her to advocate for herself by speaking to the teacher about it, and I offer my own help to assist her”; “It prepares Alexis to solve the problem herself by practicing in a low stress environment. It would make her more confident to talk to her teacher and do better in class.”\n",
    "\n",
    "-if the tutor's response does not demonstrate that the tutor understands that the student needs support in advocating for themselves, score with a 0. Sample responses scoring a 0 include: “That would be the right approach because I am helping the student solve the problem rationally\"; \"This can let him feel supported and at the same time, gives him a solution of this problem\"; \"Because it teaches Alexis a possible way to avoid similar problems in the future; and \"Discussing the plan with the student will provide them how to stand up for the inequity.\"\n",
    "\n",
    "\n",
    "Once given a response by the user, please return a JSON string following the format, {\"Rationale\": \"your reasoning here\", \"Score\":0/1}\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HH0uEw5_ythd"
   },
   "source": [
    "Feedback Elaboration Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second prompt: Pass feedback and the original input to the rewriter. Can omit the original input later to save on token cost.\n",
    "\n",
    "FEEDBACK_PROMPT_START = \"\"\"\n",
    "Please write the following third-person feedback about a tutor into the second person. In doing so, please address the tutor directly and do not exceed 100 words.\n",
    "Please start with positive feedback, if any. Subsequently, please provide any critiques, if applicable, in a constructive tone.\n",
    "\n",
    "Third-person Feedback Start ---\n",
    "\"\"\"\n",
    "\n",
    "# Design choice: Passing original input to potentially provide more concrete second-person feedback (original feedback could be high-level). Can omit to reduce cost\n",
    "ORIGINAL_RESPONSE_PROMPT = \"\"\"\n",
    "--- Third-person Feedback End. For further context, the original feedback was provided for the following response:\n",
    "\n",
    "Response Start ---\n",
    "\"\"\"\n",
    "\n",
    "FEEDBACK_FORMAT_PROMPT_WITH_RESPONSE = \"\"\"\n",
    "--- Response End. Please return your refined feedback, based on the original response and the third-person feedback, in a JSON string following the format, {\\\"Feedback\\\": \\\"your response here\\\"}.\n",
    "\"\"\"\n",
    "\n",
    "FEEDBACK_FORMAT_PROMPT_WITHOUT_RESPONSE = \"\"\"\n",
    "--- Third-person Feedback End. Please return your refined feedback, based on the third-person feedback, in a JSON string following the format, {\\\"Feedback\\\": \\\"your response here\\\"}.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VuVI8e8kxT87"
   },
   "source": [
    "Helper function for response parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_response(response_obj, json=False):\n",
    "  role = response_obj.choices[0].message.role\n",
    "  content = response_obj.choices[0].message.content\n",
    "  if json:\n",
    "    return {\"role\": role, \"content\": content}\n",
    "  else:\n",
    "    return (role, content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jIHBTOqTwW1T"
   },
   "source": [
    "## Gemini API Call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over all responses\n",
    "MAX_TOKENS = 300\n",
    "TEMPERATURE =  2\n",
    "RUN_UP_TO = 1250  # Sets a maximum index for responses to run. Useful to specify how many responses we want to run on (partial execution). Set to -1 to run them all.\n",
    "TWO_STAGE = False  # Specifies whether to refine the feedback provided by the scoring prompt\n",
    "TWO_STAGE_INCLUDE_RESPONSE = False # Specifies whether the second-stage prompt uses the original response.\n",
    "SCORE_COLUMN = 16 # Change column numbers here to  modify where output is written\n",
    "RATIONALE_COLUMN= 17\n",
    "REFINED_RATIONALE_COLUMN= 18\n",
    "\n",
    "\n",
    "MODEL = 'gemini-1.5-pro'\n",
    "#'learnlm-1.5-pro-experimental'\n",
    "\n",
    "model = genai.GenerativeModel(MODEL, system_instruction=GEMINI_SYSTEM_PROMPT)\n",
    "\n",
    "# Add titling to the new columns\n",
    "output_score_title = sheet.cell(row=1, column=SCORE_COLUMN)  # Output Score Column goes here. CHANGE COLUMN NUMBER to set score output location\n",
    "output_score_title.value = \"Gemini (few shot) Score, temp=2\"  # OPTIONAL: Sets title cell in row 1. Comment out to disable this\n",
    "\n",
    "rationale_title = sheet.cell(row=1, column=RATIONALE_COLUMN)\n",
    "rationale_title.value = \"Gemini (few shot) Rationale, temp=2\"\n",
    "\n",
    "if TWO_STAGE:  # Creates a new column for the refined rationale\n",
    "  refined_rationale_title = sheet.cell(row=1, column=REFINED_RATIONALE_COLUMN)\n",
    "  if TWO_STAGE_INCLUDE_RESPONSE:  # If the second-stage prompt also uses the original response, use a different column title\n",
    "    refined_rationale_title.value = \"Refined Feedback (w/ Response in prompt)\"\n",
    "  else:\n",
    "    refined_rationale_title.value = \"Refined Feedback (w/o response in prompt)\"\n",
    "\n",
    "\n",
    "if RUN_UP_TO >=  0:  # If an upper bound is set\n",
    "  inputs_upto = inputs[:RUN_UP_TO]\n",
    "  input_rows_upto = input_rows[:RUN_UP_TO]\n",
    "else:\n",
    "  inputs_upto = inputs  # Take the whole set of responses\n",
    "  input_rows_upto = input_rows\n",
    "\n",
    "for index, inpt in tqdm(enumerate(inputs_upto), total=len(inputs_upto)):\n",
    "#build the prompt\n",
    "  generation_prompt = \"Tutor Response: \" + inpt + \"\\n\\n. Your JSON: \" # Gemini Change\n",
    "  generation_config = genai.GenerationConfig(max_output_tokens= MAX_TOKENS, temperature=TEMPERATURE) # TODO: Identify optimal temperature. take out # on left to set tempm make sure to remove the bracket as well to prevent a syntax error\n",
    "  # Use client.chat.completions.create instead of client.generate_text\n",
    "  gemini_out = model.generate_content(generation_prompt, generation_config=generation_config) # TODO: Potential break point\n",
    "\n",
    "  # Extract the content from the response\n",
    "  content = gemini_out.text.lstrip(\"```json\")[:-5]\n",
    "  #content = gemini_out.text # Assuming extract_response is defined correctly\n",
    "  # We now need to parse the JSON into rational and score\n",
    "  score_cell = sheet.cell(row=input_rows_upto[index], column=SCORE_COLUMN) # Score cell\n",
    "  rationale_cell = sheet.cell(row=input_rows_upto[index], column=RATIONALE_COLUMN)  # Rationale cell\n",
    "  refined_feedback_cell = sheet.cell(row=input_rows_upto[index], column=REFINED_RATIONALE_COLUMN)  # Refined feedback cell\n",
    "  try:\n",
    "    content_json = json.loads(content)  # Run response through JSON\n",
    "    score = str(content_json[\"Score\"])  # Cast to string to avoid type inequality\n",
    "    rationale = str(content_json[\"Rationale\"])  # Fetch the rationale\n",
    "\n",
    "    score_cell.value = score  # Now write both into the Excel sheet\n",
    "    rationale_cell.value = rationale\n",
    "\n",
    "    # TODO: Update code to run two-stage\n",
    "    if TWO_STAGE:  # If we are to refine the feedback\n",
    "      if TWO_STAGE_INCLUDE_RESPONSE: # If we also want to pass the response, then we need to use a longer prompt, specified next:\n",
    "        refinement_history = [{\"role\": \"system\", \"content\": FEEDBACK_PROMPT_START}, {\"role\": \"user\", \"content\": rationale}, {\"role\": \"system\", \"content\": ORIGINAL_RESPONSE_PROMPT}, {\"role\": \"user\", \"content\": inpt}, {\"role\": \"system\", \"content\":FEEDBACK_FORMAT_PROMPT_WITH_RESPONSE}]\n",
    "      else: # If not, then we just pass the shorter prompt without the response\n",
    "        refinement_history = [{\"role\": \"system\", \"content\": FEEDBACK_PROMPT_START}, {\"role\": \"user\", \"content\": rationale}, {\"role\": \"system\", \"content\":FEEDBACK_FORMAT_PROMPT_WITHOUT_RESPONSE}]\n",
    "      # TODO: Switch this with Gemini call\n",
    "      gemini_ss_out = client.chat.completions.create(model=MODEL, messages=refinement_history, max_tokens=MAX_TOKENS, temperature = TEMPERATURE)  # TODO: Use different model,  temperature, max token for the second stage\n",
    "      role_ss, content_ss = extract_response(gemini_ss_out)  # ss: second stage\n",
    "      try:\n",
    "        refined_content_json = json.loads(content_ss)  #  Parse response into JSON\n",
    "        refined_feedback = str(refined_content_json[\"Feedback\"])\n",
    "        refined_feedback_cell.value = refined_feedback  # Write the refined feedback into the excel sheet.\n",
    "      except:\n",
    "        refined_feedback_cell.value = \"---\"  # If Gemini / parsing fails for whatever reason.\n",
    "  except:\n",
    "    score_cell.value = \"---\"\n",
    "    rationale_cell.value = \"---\"  # Failsafe\n",
    "    if TWO_STAGE:\n",
    "      refined_feedback_cell.value = \"---\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pvd9C9skROrl"
   },
   "source": [
    "## Save Excel Sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_NAME = \"OutputSheet_max\"+str(MAX_TOKENS)+\"tokens_temp\"+str(TEMPERATURE)  # Change file name here.\n",
    "if RUN_UP_TO >= 0:\n",
    "  FILE_NAME = FILE_NAME+\"_first\"+str(RUN_UP_TO)+\"resp\"\n",
    "if TWO_STAGE:\n",
    "  FILE_NAME = FILE_NAME+\"_twostage\"\n",
    "if TWO_STAGE_INCLUDE_RESPONSE:\n",
    "  FILE_NAME = FILE_NAME+\"_include_resp_in_prompt\"\n",
    "\n",
    "FILE_NAME = FILE_NAME + \".xlsx\"\n",
    "\n",
    "workbook.save(FILE_NAME)  # Save the new file with OpenAI output into the file system"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
