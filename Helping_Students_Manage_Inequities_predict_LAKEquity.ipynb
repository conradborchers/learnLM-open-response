{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openai  # OpenAI API for prompting GPT-4o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openpyxl\n",
    "from openai import OpenAI\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qmGkH0WztTIc"
   },
   "source": [
    "# Reading Excel Sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"Explain (2).xlsx\"\n",
    "workbook = openpyxl.load_workbook(path)\n",
    "sheet = workbook.active #[\"All_data\"]  # There is only one sheet, so no ambiguity here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5uKQIYXct0_j"
   },
   "source": [
    "Get all the responses and scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_COLUMN = 1 # Specify the column from which input is read (A->1, B->2, etc., x->24)\n",
    "APPLY_FILTER = False # Whether to filter the input rows\n",
    "row_count = 200 # sheet.max_row  # Get number of rows. If number of rows to read is already known (or sheet does not terminate where data terminates), place number here (+1) instead.\n",
    "# Filter structure column number: ([values], valid/invalid)\n",
    "INPUT_FILTER_DICTIONARY = {36: ([\"Mixed\"], False), 4: ([\n",
    "                               \"1. What exactly would you say OR how exactly would you respond in order to<span  style='color: #0000ff;'> </span>help Alexis manage the inequity related to her learning by assisting her to advocate for herself?<br/>\",\n",
    "                               \"1. What exactly would you say OR how exactly would you respond in order to<span  style='color: #0000ff;'> </span>help Jeremiah manage the inequity related to his learning by assisting him to advocate for himself?<br/>\",\n",
    "                               \"9.  What exactly would you say OR how exactly would you respond in order to<span  style='color: #0000ff;'> </span>help Alexis manage the inequity related to her learning by assisting her to advocate for herself?<br/>\",\n",
    "                               \"9. What exactly would you say OR how exactly would you respond in order to<span  style='color: #0000ff;'> </span>help Jeremiah manage the inequity related to his learning by assisting him to advocate for himself?<br/>\"],\n",
    "                                True)}\n",
    "# Function to check if a row matches the filter criteria\n",
    "def matches_filters(sheet, row, filter_dict):\n",
    "    for col, (valid_values, is_valid) in filter_dict.items():\n",
    "        cell_value = sheet.cell(row=row, column=col).value\n",
    "        if (cell_value in valid_values) != is_valid:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "# Read the input data\n",
    "inputs = []\n",
    "for i in range(2, row_count + 1):\n",
    "    if not APPLY_FILTER or matches_filters(sheet, i, INPUT_FILTER_DICTIONARY):\n",
    "        cell_value = sheet.cell(row=i, column=INPUT_COLUMN).value\n",
    "        if cell_value is not None:\n",
    "            inputs.append(cell_value)\n",
    "\n",
    "# Now `inputs` contains only the filtered data\n",
    "#inputs = [sheet.cell(row=i, column=INPUT_COLUMN).value  for i in range(2, row_count + 1)]  # Column number is set here. YOUR INPUT COLUMN NUMBER GOES HERE.\n",
    "#inputs = [x for x in inputs if x is not None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qASEYdU1vH08"
   },
   "source": [
    "# OpenAI and Prompt Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = \"\" # Insert your API Key here.\n",
    "client = OpenAI(api_key=API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HhcWKgsOP_gx"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cKg2bQ7qvi_i"
   },
   "source": [
    "Scoring Prompt Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCORING_PROMPT_START = \"\"\"\n",
    "You are a tutor evaluator. Please assess a tutor’s response within a tutor training scenario involving a tutor instructing a middle school student to advocate for themselves when they are experiencing an inequity. Educators and tutors should apply the strategy of assisting students in identifying when they are experiencing an inequity and instruct them to advocate for themselves. The tutor is explaining the rationale behind their response. Assess and score the tutor’s response, as follows:\n",
    "\n",
    "\n",
    "-if the tutor’s response demonstrates that they recognize that the student needs support in advocating for themselves and encourages the student to act, score with a 1. Sample responses scoring a 1 include: “It practices critical hope. It encourages her to advocate for herself by speaking to the teacher about it, and I offer my own help to assist her”; “It prepares Alexis to solve the problem herself by practicing in a low stress environment. It would make her more confident to talk to her teacher and do better in class.”\n",
    "\n",
    "\n",
    "-if the tutor's response does not demonstrate that the tutor understands that the student needs support in advocating for themselves, score with a 0. Sample responses scoring a 0 include: “That would be the right approach because I am helping the student solve the problem rationally\"; \"This can let him feel supported and at the same time, gives him a solution of this problem\"; \"Because it teaches Alexis a possible way to avoid similar problems in the future; and \"Discussing the plan with the student will provide them how to stand up for the inequity.\"\n",
    "\n",
    "\n",
    "Response Start ---\n",
    "\"\"\"\n",
    "\n",
    "SCORING_FORMAT_PROMPT = \"\"\"\n",
    "--- Response End. Given the earlier response, please return a JSON string following the format, {\"Rationale\": \"your reasoning here\", \"Score\":0/1}.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HH0uEw5_ythd"
   },
   "source": [
    "Feedback Elaboration Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second prompt: Pass feedback and the original input to the rewriter. Can omit the original input later to save on token cost.\n",
    "\n",
    "FEEDBACK_PROMPT_START = \"\"\"\n",
    "Please write the following third-person feedback about a tutor into the second person. In doing so, please address the tutor directly and do not exceed 100 words.\n",
    "Please start with positive feedback, if any. Subsequently, please provide any critiques, if applicable, in a constructive tone.\n",
    "\n",
    "Third-person Feedback Start ---\n",
    "\"\"\"\n",
    "\n",
    "# Design choice: Passing original input to potentially provide more concrete second-person feedback (original feedback could be high-level). Can omit to reduce cost\n",
    "ORIGINAL_RESPONSE_PROMPT = \"\"\"\n",
    "--- Third-person Feedback End. For further context, the original feedback was provided for the following response:\n",
    "\n",
    "Response Start ---\n",
    "\"\"\"\n",
    "\n",
    "FEEDBACK_FORMAT_PROMPT_WITH_RESPONSE = \"\"\"\n",
    "--- Response End. Please return your refined feedback, based on the original response and the third-person feedback, in a JSON string following the format, {\\\"Feedback\\\": \\\"your response here\\\"}.\n",
    "\"\"\"\n",
    "\n",
    "FEEDBACK_FORMAT_PROMPT_WITHOUT_RESPONSE = \"\"\"\n",
    "--- Third-person Feedback End. Please return your refined feedback, based on the third-person feedback, in a JSON string following the format, {\\\"Feedback\\\": \\\"your response here\\\"}.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VuVI8e8kxT87"
   },
   "source": [
    "Helper function for response parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_response(response_obj, json=False):\n",
    "  role = response_obj.choices[0].message.role\n",
    "  content = response_obj.choices[0].message.content\n",
    "  if json:\n",
    "    return {\"role\": role, \"content\": content}\n",
    "  else:\n",
    "    return (role, content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jIHBTOqTwW1T"
   },
   "source": [
    "## OpenAI API Call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over all responses\n",
    "MAX_TOKENS = 300\n",
    "TEMPERATURE = 0\n",
    "RUN_UP_TO = 200  # Sets a maximum index for responses to run. Useful to specify how many responses we want to run on (partial execution). Set to -1 to run them all.\n",
    "TWO_STAGE = False  # Specifies whether to refine the feedback provided by the scoring prompt\n",
    "TWO_STAGE_INCLUDE_RESPONSE = False # Specifies whether the second-stage prompt uses the original response.\n",
    "SCORE_COLUMN = 2 # Change column numbers here to  modify where output is written\n",
    "RATIONALE_COLUMN= 3\n",
    "REFINED_RATIONALE_COLUMN= 4\n",
    "\n",
    "\n",
    "MODEL = \"gpt-4-turbo\"\n",
    "\n",
    "# Add titling to the new columns\n",
    "output_score_title = sheet.cell(row=1, column=SCORE_COLUMN)  # Output Score Column goes here. CHANGE COLUMN NUMBER to set score output location\n",
    "output_score_title.value = \"GPT-4-turbo Score\"  # OPTIONAL: Sets title cell in row 1. Comment out to disable this\n",
    "\n",
    "rationale_title = sheet.cell(row=1, column=RATIONALE_COLUMN)\n",
    "rationale_title.value = \"GPT-4-turbo Rationale\"\n",
    "\n",
    "if TWO_STAGE:  # Creates a new column for the refined rationale\n",
    "  refined_rationale_title = sheet.cell(row=1, column=REFINED_RATIONALE_COLUMN)\n",
    "  if TWO_STAGE_INCLUDE_RESPONSE:  # If the second-stage prompt also uses the original response, use a different column title\n",
    "    refined_rationale_title.value = \"Refined Feedback (w/ Response in prompt)\"\n",
    "  else:\n",
    "    refined_rationale_title.value = \"Refined Feedback (w/o response in prompt)\"\n",
    "\n",
    "\n",
    "if RUN_UP_TO >=  0:  # If an upper bound is set\n",
    "  inputs_upto = inputs[:RUN_UP_TO]\n",
    "else:\n",
    "  inputs_upto = inputs  # Take the whole set of responses\n",
    "\n",
    "for index, inpt in tqdm(enumerate(inputs_upto), total=len(inputs_upto)):\n",
    "  overall_history = [{\"role\": \"system\", \"content\": SCORING_PROMPT_START}, {\"role\": \"user\", \"content\": inpt}, {\"role\": \"system\", \"content\": SCORING_FORMAT_PROMPT}]\n",
    "  openai_out = client.chat.completions.create(model=MODEL, messages=overall_history, max_tokens=MAX_TOKENS, temperature = TEMPERATURE)\n",
    "  role, content = extract_response(openai_out)\n",
    "  # We now need to parse the JSON into rational and score\n",
    "  score_cell = sheet.cell(row=index+2, column=SCORE_COLUMN) # Score cell\n",
    "  rationale_cell = sheet.cell(row=index+2, column=RATIONALE_COLUMN)  # Rationale cell\n",
    "  refined_feedback_cell = sheet.cell(row=index+2, column=REFINED_RATIONALE_COLUMN)  # Refined feedback cell\n",
    "  try:\n",
    "    content_json = json.loads(content)  # Run response through JSON\n",
    "    score = str(content_json[\"Score\"])  # Cast to string to avoid type inequality\n",
    "    rationale = str(content_json[\"Rationale\"])  # Fetch the rationale\n",
    "\n",
    "    score_cell.value = score  # Now write both into the Excel sheet\n",
    "    rationale_cell.value = rationale\n",
    "\n",
    "    if TWO_STAGE:  # If we are to refine the feedback\n",
    "      if TWO_STAGE_INCLUDE_RESPONSE: # If we also want to pass the response, then we need to use a longer prompt, specified next:\n",
    "        refinement_history = [{\"role\": \"system\", \"content\": FEEDBACK_PROMPT_START}, {\"role\": \"user\", \"content\": rationale}, {\"role\": \"system\", \"content\": ORIGINAL_RESPONSE_PROMPT}, {\"role\": \"user\", \"content\": inpt}, {\"role\": \"system\", \"content\":FEEDBACK_FORMAT_PROMPT_WITH_RESPONSE}]\n",
    "      else: # If not, then we just pass the shorter prompt without the response\n",
    "        refinement_history = [{\"role\": \"system\", \"content\": FEEDBACK_PROMPT_START}, {\"role\": \"user\", \"content\": rationale}, {\"role\": \"system\", \"content\":FEEDBACK_FORMAT_PROMPT_WITHOUT_RESPONSE}]\n",
    "      openai_ss_out = client.chat.completions.create(model=MODEL, messages=refinement_history, max_tokens=MAX_TOKENS, temperature = TEMPERATURE)  # TODO: Use different model,  temperature, max token for the second stage\n",
    "      role_ss, content_ss = extract_response(openai_ss_out)  # ss: second stage\n",
    "      try:\n",
    "        refined_content_json = json.loads(content_ss)  #  Parse response into JSON\n",
    "        refined_feedback = str(refined_content_json[\"Feedback\"])\n",
    "        refined_feedback_cell.value = refined_feedback  # Write the refined feedback into the excel sheet.\n",
    "      except:\n",
    "        refined_feedback_cell.value = \"---\"  # If OpenAI / parsing fails for whatever reason.\n",
    "  except:\n",
    "    score_cell.value = \"---\"\n",
    "    rationale_cell.value = \"---\"  # Failsafe\n",
    "    if TWO_STAGE:\n",
    "      refined_feedback_cell.value = \"---\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pvd9C9skROrl"
   },
   "source": [
    "## Save Excel Sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_NAME = \"OutputSheet_max\"+str(MAX_TOKENS)+\"tokens_temp\"+str(TEMPERATURE)  # Change file name here.\n",
    "if RUN_UP_TO >= 0:\n",
    "  FILE_NAME = FILE_NAME+\"_first\"+str(RUN_UP_TO)+\"resp\"\n",
    "if TWO_STAGE:\n",
    "  FILE_NAME = FILE_NAME+\"_twostage\"\n",
    "if TWO_STAGE_INCLUDE_RESPONSE:\n",
    "  FILE_NAME = FILE_NAME+\"_include_resp_in_prompt\"\n",
    "\n",
    "FILE_NAME = FILE_NAME + \".xlsx\"\n",
    "\n",
    "workbook.save(FILE_NAME)  # Save the new file with OpenAI output into the file system"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
